name = "train_plksr_tiny_3x"
model_type = "image"
scale = 3
use_amp = true
bfloat16 = true
fast_matmul = true
#compile = true

[datasets.train]
type = "paired"
dataroot_gt = '/teamspace/studios/upres-ml-dataset-small/sr_dataset/3_0x/train/HR'
dataroot_lq = '/teamspace/studios/upres-ml-dataset-small/sr_dataset/3_0x/train/LR'
patch_size = 32
batch_size = 4
accumulate = 1
hflip = true
rotation = true
num_worker_per_gpu = "auto"
prefetch_factor = 2  # Number of batches loaded in advance per worker
pin_memory = true   # Pins CPU memory for faster transfer to GPU
persistent_workers = true  # Keeps workers alive between iterations

[datasets.val]
name = "val"
type = "paired"
dataroot_gt = '/teamspace/studios/upres-ml-dataset-small/sr_dataset/3_0x/val/HR'
dataroot_lq = '/teamspace/studios/upres-ml-dataset-small/sr_dataset/3_0x/val/LR'

[val]
val_freq = 5000

[val.metrics.psnr]
type = "calculate_psnr"
[val.metrics.ssim]
type = "calculate_ssim"


[path]
pretrain_network_g = '/teamspace/studios/this_studio/plksr_2x_to_3x_init.pth'
strict_load_g = false

[network_g]
type = "plksr_tiny"

[network_d]
type = "metagan"

[train]
wavelet_guided = true
wavelet_init = 80000
resume_state = false
# Stability settings
ema = 0.999
grad_clip = true  # Enable gradient clipping
grad_clip_norm = 0.5  # More aggressive gradient clipping
clamp = true  # Clamp pixel values to valid range
# Gradual learning rate warmup can help stability
#warmup_iter = 1000  # Warm up learning rate over 1000 iterations


[train.optim_g]
type = "adamw"  # AdamW tends to be more stable than SGD
lr = 1e-5  # Very conservative learning rate for fine-tuning
weight_decay = 1e-4  # Mild weight decay helps regularization
betas = [0.9, 0.99]  # Higher beta2 for more stable momentum estimates

[train.optim_d]
type = "adam"
lr = 1e-4
betas = [ 0.9, 0.99 ]

# Enable these losses to satisfy NeoSR's requirements
# L1 loss (primary loss in the paper)
[train.l1_opt]
type = "l1_loss"
loss_weight = 1.0

# Include mssim as a pixel loss to satisfy the error message
[train.mssim_opt]
type = "mssim_loss"
loss_weight = 0.1  # Keep weight low to prioritize L1 loss

# Keep adversarial loss (GAN)
[train.gan_opt]
type = "gan_loss"
gan_type = "bce"
loss_weight = 0.3

[logger]
total_iter = 450000
save_checkpoint_freq = 5000
use_tb_logger = true